{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Beautiful Soup\n",
    "\n",
    "* * * \n",
    "\n",
    "### Icons used in this notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "ü•ä **Challenge**: Interactive exercise. We'll work through these in the workshop!<br>\n",
    "‚ö†Ô∏è **Warning**: Heads-up about tricky stuff or common mistakes.<br>\n",
    "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "üé¨ **Demo**: Showing off something more advanced ‚Äì so you know what Python can be used for!<br>\n",
    "\n",
    "### Learning Objectives\n",
    "1. [Reflection: To Scape Or Not To Scrape](#when)\n",
    "2. [Extracting and Parsing HTML](#extract)\n",
    "3. [Scraping the Illinois General Assembly](#scrape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='when'></a>\n",
    "\n",
    "# To Scrape Or Not To Scrape\n",
    "\n",
    "When we'd like to access data from the web, we first have to make sure if the website we are interested in offers a Web API. Platforms like Twitter, Reddit, and the New York Times offer APIs. **Check out D-Lab's [Python Web APIs](https://github.com/dlab-berkeley/Python-Web-APIs) workshop if you want to learn how to use APIs.**\n",
    "\n",
    "However, there are often cases when a Web API does not exist. In these cases, we may have to resort to web scraping, where we extract the underlying HTML from a web page, and directly obtain the information we want. There are several packages in Python we can use to accomplish these tasks. We'll focus two packages: Requests and Beautiful Soup.\n",
    "\n",
    "Our case study will be scraping information on the [state senators of Illinois](http://www.ilga.gov/senate), as well as the [list of bills](http://www.ilga.gov/senate/SenatorBills.asp?MemberID=1911&GA=98&Primary=True) each senator has sponsored. Before we get started, peruse these websites to take a look at their structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "We will use two main packages: [Requests](http://docs.python-requests.org/en/latest/user/quickstart/) and [Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/bs4/doc/). Go ahead and install these packages, if you haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\ymollocano\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ymollocano\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ymollocano\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ymollocano\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ymollocano\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2025.7.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests     \n",
    "# Este script instala la biblioteca requests usando pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ymollocano\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.13.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ymollocano\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\ymollocano\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4) (4.14.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install beautifulsoup4\n",
    "# Este script instala la biblioteca BeautifulSoup4 usando pip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also install the `lxml` package, which helps support some of the parsing that Beautiful Soup performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\ymollocano\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (6.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install lxml\n",
    "# Este script instala la biblioteca lxml usando pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import time\n",
    "# # Importa las bibliotecas necesarias para el web scraping y el manejo de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='extract'></a>\n",
    "\n",
    "# Extracting and Parsing HTML \n",
    "\n",
    "In order to succesfully scrape and analyse HTML, we'll be going through the following 4 steps:\n",
    "1. Make a GET request\n",
    "2. Parse the page with Beautiful Soup\n",
    "3. Search for HTML elements\n",
    "4. Get attributes and text of these elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Make a GET Request to Obtain a Page's HTML\n",
    "\n",
    "We can use the Requests library to:\n",
    "\n",
    "1. Make a GET request to the page, and\n",
    "2. Read in the webpage's HTML code.\n",
    "\n",
    "The process of making a request and obtaining a result resembles that of the Web API workflow. Now, however, we're making a request directly to the website, and we're going to have to parse the HTML ourselves. This is in contrast to being provided data organized into a more straightforward `JSON` or `XML` output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head id=\"Head1\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
      "    <meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\" />\n",
      "    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\" />\n",
      "    <meta charset=\"utf-8\" />\n",
      "    <meta charset=\"UTF-8\">\n",
      "    <!-- Meta Description -->\n",
      "    <meta name=\"description\" content=\"Welcome to the official government website of the Illinois General Assembly\">\n",
      "    <meta name=\"contactName\" content=\"Legislative Information System\">\n",
      "    <meta name=\"contactOrganization\" content=\"LIS Staff Services\">\n",
      "    <meta name=\"contactStreetAddress1\" content=\"705 Stratton Office Building\">\n",
      "    <meta name=\"contactCity\" content=\"Springfield\">\n",
      "    <meta name=\"contactZipcode\" content=\"62706\">\n",
      "    <meta name=\"contactNetworkAddress\" content=\"webmaster@ilga.gov\">\n",
      "    <meta name=\"contactPhoneNumber\" content=\"217-782-3944\">\n",
      "    <meta name=\"contactFaxNumber\" content=\"217-524-6059\">\n",
      "    <meta name\n"
     ]
    }
   ],
   "source": [
    "# Realiza una solicitud GET\n",
    "req = requests.get('http://www.ilga.gov/senate/default.asp')\n",
    "# Lee el contenido de la respuesta del servidor\n",
    "src = req.text\n",
    "# Muestra una parte de la salida\n",
    "print(src[:1000])\n",
    "# # Realiza una solicitud GET a la p√°gina del Senado de Illinois y muestra los primeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Parse the Page with Beautiful Soup\n",
    "\n",
    "Now, we use the `BeautifulSoup` function to parse the reponse into an HTML tree. This returns an object (called a **soup object**) which contains all of the HTML in the original document.\n",
    "\n",
    "If you run into an error about a parser library, make sure you've installed the `lxml` package to provide Beautiful Soup with the necessary parsing tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head id=\"Head1\">\n",
      "  <meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\n",
      "  <meta content=\"text/html;charset=utf-8\" http-equiv=\"content-type\"/>\n",
      "  <meta content=\"IE=Edge\" http-equiv=\"X-UA-Compatible\"/>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <!-- Meta Description -->\n",
      "  <meta content=\"Welcome to the official government website of the Illinois General Assembly\" name=\"description\"/>\n",
      "  <meta content=\"Legislative Information System\" name=\"contactName\"/>\n",
      "  <meta content=\"LIS Staff Services\" name=\"contactOrganization\"/>\n",
      "  <meta content=\"705 Stratton Office Building\" name=\"contactStreetAddress1\"/>\n",
      "  <meta content=\"Springfield\" name=\"contactCity\"/>\n",
      "  <meta content=\"62706\" name=\"contactZipcode\"/>\n",
      "  <meta content=\"webmaster@ilga.gov\" name=\"contactNetworkAddress\"/>\n",
      "  <meta content=\"217-782-3944\" name=\"contactPhoneNumber\"/>\n",
      "  <meta content=\"217-524-6059\" name=\"contactFaxNumber\"/>\n",
      "  <meta content=\"State Of Illinois\" name=\"originatorJur\n"
     ]
    }
   ],
   "source": [
    "# Analiza la respuesta y la convierte en un √°rbol HTML\n",
    "soup = BeautifulSoup(src, 'lxml')\n",
    "# Muestra los primeros 1000 caracteres del HTML formateado\n",
    "print(soup.prettify()[:1000])\n",
    "# BeautifulSoup(src, 'lxml') toma el contenido HTML (src) y lo analiza usando el parser lxml, creando un objeto llamado soup que representa la estructura del documento HTML.\n",
    "# soup.prettify() devuelve el HTML con una indentaci√≥n legible, facilitando la visualizaci√≥n de la estructura.\n",
    "# Al imprimir solo los primeros 1000 caracteres, puedes ver una muestra del HTML procesado, lo que ayuda a identificar las etiquetas y clases necesarias para extraer informaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output looks pretty similar to the above, but now it's organized in a `soup` object which allows us to more easily traverse the page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Search for HTML Elements\n",
    "\n",
    "Beautiful Soup has a number of functions to find useful components on a page. Beautiful Soup lets you find elements by their:\n",
    "\n",
    "1. HTML tags\n",
    "2. HTML Attributes\n",
    "3. CSS Selectors\n",
    "\n",
    "Let's search first for **HTML tags**. \n",
    "\n",
    "The function `find_all` searches the `soup` tree to find all the elements with an a particular HTML tag, and returns all of those elements.\n",
    "\n",
    "What does the example below do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"en\" href=\"#\">\n",
      "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-us\"></span> English\n",
      "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"af\" href=\"#\">\n",
      "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-za\"></span> Afrikaans\n",
      "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"sq\" href=\"#\">\n",
      "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-al\"></span> Albanian\n",
      "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"ar\" href=\"#\">\n",
      "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-ae\"></span> Arabic\n",
      "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"hy\" href=\"#\">\n",
      "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-am\"></span> Armenian\n",
      "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"az\" href=\"#\">\n",
      "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-az\"></span> Azerbaijani\n",
      "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"eu\" href=\"#\">\n",
      "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-eu\"></span> Basque\n",
      "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"bn\" href=\"#\">\n",
      "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-bd\"></span> Bengali\n",
      "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"bs\" href=\"#\">\n",
      "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-ba\"></span> Bosnian\n",
      "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"ca\" href=\"#\">\n",
      "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-es\"></span> Catalan\n",
      "                            </a>]\n"
     ]
    }
   ],
   "source": [
    "# Busca todos los elementos con la etiqueta 'a' (enlaces)\n",
    "a_tags = soup.find_all(\"a\")\n",
    "# Muestra los primeros 10 enlaces encontrados\n",
    "print(a_tags[:10])\n",
    "# Busca todos los enlaces en el documento HTML y muestra los primeros 10 encontrados.\n",
    "# soup.find_all(\"a\") busca y devuelve una lista con todos los elementos <a> (enlaces) del documento HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `find_all()` is the most popular method in the Beautiful Soup search API, you can use a shortcut for it. If you treat the BeautifulSoup object as though it were a function, then it‚Äôs the same as calling `find_all()` on that object. \n",
    "\n",
    "These two lines of code are equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"en\" href=\"#\">\n",
      "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-us\"></span> English\n",
      "                            </a>\n",
      "<a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"en\" href=\"#\">\n",
      "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-us\"></span> English\n",
      "                            </a>\n"
     ]
    }
   ],
   "source": [
    "# Ambas l√≠neas buscan todos los elementos con la etiqueta 'a' (enlaces)\n",
    "a_tags = soup.find_all(\"a\")\n",
    "a_tags_alt = soup(\"a\")  # Equivalente a find_all(\"a\")\n",
    "\n",
    "# Imprime el primer enlace encontrado usando ambos m√©todos\n",
    "print(a_tags[0])\n",
    "print(a_tags_alt[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many links did we obtain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270\n"
     ]
    }
   ],
   "source": [
    "# Imprime la cantidad total de enlaces <a> encontrados en el documento HTML\n",
    "print(len(a_tags))\n",
    "#Esto muestra cu√°ntos elementos <a> (enlaces) hay en la p√°gina, lo que te da una idea de la cantidad de hiperv√≠nculos presentes en el HTML analizado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot! Many elements on a page will have the same HTML tag. For instance, if you search for everything with the `a` tag, you're likely to get more hits, many of which you might not want. Remember, the `a` tag defines a hyperlink, so you'll usually find many on any given page.\n",
    "\n",
    "What if we wanted to search for HTML tags with certain attributes, such as particular CSS classes? \n",
    "\n",
    "We can do this by adding an additional argument to the `find_all`. In the example below, we are finding all the `a` tags, and then filtering those with `class_=\"sidemenu\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Obtiene solo los elementos 'a' con la clase 'sidemenu'\n",
    "side_menus = soup(\"a\", class_=\"sidemenu\")\n",
    "# Muestra los primeros 5 enlaces encontrados con la clase 'sidemenu'\n",
    "print(side_menus[:5])\n",
    "# Obtiene todos los enlaces con la clase 'sidemenu' y muestra los primeros 5 encontrados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more efficient way to search for elements on a website is via a **CSS selector**. For this we have to use a different method called `select()`. Just pass a string into the `.select()` to get all elements with that string as a valid CSS selector.\n",
    "\n",
    "In the example above, we can use `\"a.sidemenu\"` as a CSS selector, which returns all `a` tags with class `sidemenu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Obtiene los elementos con el selector CSS \"a.sidemenu\"\n",
    "selected = soup.select(\"a.sidemenu\")\n",
    "# Muestra los primeros 5 elementos encontrados\n",
    "print(selected[:5])\n",
    "#soup.select(\"a.sidemenu\") devuelve todos los elementos <a> que tienen la clase sidemenu usando un selector CSS.\n",
    "#print(selected[:5]) muestra los primeros 5 resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge: Find All\n",
    "\n",
    "Use BeautifulSoup to find all the `a` elements with class `mainmenu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Encontrar todos los elementos 'a' con clase 'mainmenu'\n",
    "elementos_mainmenu = soup.find_all('a', class_='mainmenu')\n",
    "\n",
    "# Sintaxis alternativa (ambas funcionan igual):\n",
    "# elementos_mainmenu = soup.find_all('a', {'class': 'mainmenu'})\n",
    "\n",
    "print(f\"Se encontraron {len(elementos_mainmenu)} elementos con clase 'mainmenu'\")\n",
    "\n",
    "# Imprimir cada elemento\n",
    "for i, elemento in enumerate(elementos_mainmenu):\n",
    "    print(f\"Elemento {i + 1}: {elemento}\")\n",
    "    \n",
    "# Si quieres obtener el contenido de texto de cada elemento:\n",
    "for elemento in elementos_mainmenu:\n",
    "    print(f\"Texto: {elemento.text}\")\n",
    "    \n",
    "# Si quieres obtener atributos espec√≠ficos (como href):\n",
    "for elemento in elementos_mainmenu:\n",
    "    if elemento.has_attr('href'):\n",
    "        print(f\"Enlace: {elemento['href']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Get Attributes and Text of Elements\n",
    "\n",
    "Once we identify elements, we want the access information in that element. Usually, this means two things:\n",
    "\n",
    "1. Text\n",
    "2. Attributes\n",
    "\n",
    "Getting the text inside an element is easy. All we have to do is use the `text` member of a `tag` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m side_menu_links = soup.select(\u001b[33m\"\u001b[39m\u001b[33ma.sidemenu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Examine the first link\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m first_link = \u001b[43mside_menu_links\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(first_link)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# What class is this variable?\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Obtiene todos los enlaces con la clase 'sidemenu' como una lista\n",
    "side_menu_links = soup.select(\"a.sidemenu\")\n",
    "\n",
    "# Examina el primer enlace\n",
    "first_link = side_menu_links[0]\n",
    "print(first_link)\n",
    "\n",
    "# ¬øQu√© clase tiene esta variable?\n",
    "print('Clase: ', type(first_link))\n",
    "# Este c√≥digo selecciona todos los elementos <a> que tienen la clase 'sidemenu' usando un selector CSS.\n",
    "# Luego, toma el primer enlace de esa lista y lo imprime para mostrar su estructura HTML.\n",
    "# Despu√©s, imprime el tipo de la variable 'first_link', que ser√° <class 'bs4.element.Tag'>,\n",
    "# indicando que es un objeto Tag de BeautifulSoup, lo que permite acceder f√°cilmente a sus atributos y contenido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a Beautiful Soup tag! This means it has a `text` member:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imprime el texto contenido en el primer enlace con la clase 'sidemenu'\n",
    "print(first_link.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we want the value of certain attributes. This is particularly relevant for `a` tags, or links, where the `href` attribute tells us where the link goes.\n",
    "\n",
    "üí° **Tip**: You can access a tag‚Äôs attributes by treating the tag like a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imprime el valor del atributo 'href' del primer enlace con la clase 'sidemenu'\n",
    "print(first_link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge: Extract specific attributes\n",
    "\n",
    "Extract all `href` attributes for each `mainmenu` URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scrape'></a>\n",
    "\n",
    "# Scraping the Illinois General Assembly\n",
    "\n",
    "Believe it or not, those are really the fundamental tools you need to scrape a website. Once you spend more time familiarizing yourself with HTML and CSS, then it's simply a matter of understanding the structure of a particular website and intelligently applying the tools of Beautiful Soup and Python.\n",
    "\n",
    "Let's apply these skills to scrape the [Illinois 98th General Assembly](http://www.ilga.gov/senate/default.asp?GA=98).\n",
    "\n",
    "Specifically, our goal is to scrape information on each senator, including their name, district, and party."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape and Soup the Webpage\n",
    "\n",
    "Let's scrape and parse the webpage, using the tools we learned in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Realiza una solicitud GET a la p√°gina del Senado de Illinois para la 98¬™ Asamblea General\n",
    "req = requests.get('http://www.ilga.gov/senate/default.asp?GA=98')\n",
    "  # Solicita el contenido HTML de la p√°gina web\n",
    "\n",
    "# Lee el contenido de la respuesta del servidor (HTML de la p√°gina)\n",
    "src = req.text  # Extrae el texto (HTML) de la respuesta\n",
    "\n",
    "# Analiza el HTML usando BeautifulSoup con el parser 'lxml'\n",
    "soup = BeautifulSoup(src, \"lxml\")  \n",
    "# Convierte el HTML en un objeto BeautifulSoup para facilitar su an√°lisis y extracci√≥n de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for the Table Elements\n",
    "\n",
    "Our goal is to obtain the elements in the table on the webpage. Remember: rows are identified by the `tr` tag. Let's use `find_all` to obtain these elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtiene todos los elementos de fila de la tabla (<tr>) del objeto soup\n",
    "rows = soup.find_all(\"tr\")\n",
    "# Imprime la cantidad total de filas encontradas en la tabla\n",
    "len(rows)\n",
    "# Este c√≥digo obtiene todas las filas (<tr>) de la tabla HTML y muestra cu√°ntas filas hay en total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning**: Keep in mind: `find_all` gets *all* the elements with the `tr` tag. We only want some of them. If we use the 'Inspect' function in Google Chrome and look carefully, then we can use some CSS selectors to get just the rows we're interested in. Specifically, we want the inner rows of the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devuelve todos los elementos que coinciden con el selector CSS 'tr tr tr' en la p√°gina\n",
    "rows = soup.select('tr tr tr')\n",
    "\n",
    "# Imprime las primeras 5 filas encontradas\n",
    "for row in rows[:5]:\n",
    "    print(row, '\\n')\n",
    "\n",
    "# Este c√≥digo selecciona todas las filas anidadas (tr dentro de tr dentro de tr) en la tabla HTML y muestra las primeras 5, lo que ayuda a inspeccionar la estructura de los datos extra√≠dos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we want everything after the first two rows. Let's work with a single row to start, and build our loop from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecciona la tercera fila relevante de la tabla (usando el √≠ndice 2)\n",
    "example_row = rows[2]\n",
    "# Imprime la representaci√≥n formateada (indentada) en HTML de esa fila para facilitar su an√°lisis\n",
    "print(example_row.prettify())\n",
    "# Este c√≥digo selecciona una fila espec√≠fica de la tabla HTML y muestra su contenido estructurado para examinar su formato y etiquetas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break this row down into its component cells/columns using the `select` method with CSS selectors. Looking closely at the HTML, there are a couple of ways we could do this.\n",
    "\n",
    "* We could identify the cells by their tag `td`.\n",
    "* We could use the the class name `.detail`.\n",
    "* We could combine both and use the selector `td.detail`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Itera sobre todas las celdas <td> en la fila de ejemplo y las imprime\n",
    "for cell in example_row.select('td'):\n",
    "    print(cell)\n",
    "print()\n",
    "# Recorre e imprime todas las celdas de tipo <td> en la fila seleccionada.\n",
    "\n",
    "# Itera sobre todas las celdas con la clase 'detail' en la fila de ejemplo y las imprime\n",
    "for cell in example_row.select('.detail'):\n",
    "    print(cell)\n",
    "print()\n",
    "# Recorre e imprime todas las celdas que tienen la clase 'detail' en la fila seleccionada.\n",
    "\n",
    "# Itera sobre todas las celdas <td> que tienen la clase 'detail' en la fila de ejemplo y las imprime\n",
    "for cell in example_row.select('td.detail'):\n",
    "    print(cell)\n",
    "print()\n",
    "# Recorre e imprime todas las celdas <td> que adem√°s tienen la clase 'detail' en la fila seleccionada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that these are all the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verifica que seleccionar todas las celdas <td>, todas las celdas con clase 'detail' y todas las celdas <td> con clase 'detail' en la fila de ejemplo\n",
    "# devuelve exactamente la misma lista de elementos. Esto asegura que los tres m√©todos de selecci√≥n son equivalentes en este caso.\n",
    "assert example_row.select('td') == example_row.select('.detail') == example_row.select('td.detail')\n",
    "# El c√≥digo comprueba que los tres m√©todos de selecci√≥n obtienen exactamente los mismos elementos de la fila de ejemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the selector `td.detail` to be as specific as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecciona solo aquellas celdas 'td' que tienen la clase 'detail' en la fila de ejemplo\n",
    "detail_cells = example_row.select('td.detail')\n",
    "detail_cells  # Devuelve una lista de celdas <td> con clase 'detail' de la fila seleccionada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, we're interested in the actual **text** of a website, not its tags. Recall that to get the text of an HTML element, we use the `text` member:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conserva solo el texto de cada una de las celdas 'detail'\n",
    "row_data = [cell.text for cell in detail_cells]\n",
    "\n",
    "# Imprime la lista con los textos extra√≠dos de las celdas\n",
    "print(row_data)\n",
    "# Este c√≥digo extrae el texto de cada celda con clase 'detail' en una fila y lo muestra como una lista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Now we just use our basic Python knowledge to get the elements of this list that we want. Remember, we want the senator's name, their district, and their party."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime el nombre del senador (primer elemento de row_data)\n",
    "print(row_data[0]) # Nombre\n",
    "# Muestra el nombre del senador extra√≠do de la fila\n",
    "\n",
    "# Imprime el distrito del senador (cuarto elemento de row_data)\n",
    "print(row_data[3]) # Distrito\n",
    "# Muestra el n√∫mero de distrito del senador extra√≠do de la fila\n",
    "\n",
    "# Imprime el partido del senador (quinto elemento de row_data)\n",
    "print(row_data[4]) # Partido\n",
    "# Muestra el partido pol√≠tico del senador extra√≠do de la fila"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Rid of Junk Rows\n",
    "\n",
    "We saw at the beginning that not all of the rows we got actually correspond to a senator. We'll need to do some cleaning before we can proceed forward. Take a look at some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime la primera fila (√≠ndice 0) de la lista 'rows'\n",
    "print('Row 0:\\n', rows[0], '\\n')\n",
    "# Imprime la segunda fila (√≠ndice 1) de la lista 'rows'\n",
    "print('Row 1:\\n', rows[1], '\\n')\n",
    "# Imprime la √∫ltima fila de la lista 'rows'\n",
    "print('Last Row:\\n', rows[-1])\n",
    "\n",
    "# Este c√≥digo muestra por pantalla el contenido de la primera, segunda y √∫ltima fila de la lista 'rows', permitiendo inspeccionar su estructura y contenido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we write our for loop, we only want it to apply to the relevant rows. So we'll need to filter out the irrelevant rows. The way to do this is to compare some of these to the rows we do want, see how they differ, and then formulate that in a conditional.\n",
    "\n",
    "As you can imagine, there a lot of possible ways to do this, and it'll depend on the website. We'll show some here to give you an idea of how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filas no deseadas (bad rows)\n",
    "print(len(rows[0]))   # Imprime la longitud de la primera fila (probablemente no relevante)\n",
    "print(len(rows[1]))   # Imprime la longitud de la segunda fila (probablemente no relevante)\n",
    "\n",
    "# Filas deseadas (good rows)\n",
    "print(len(rows[2]))   # Imprime la longitud de la tercera fila (probablemente relevante)\n",
    "print(len(rows[3]))   # Imprime la longitud de la cuarta fila (probablemente relevante)\n",
    "\n",
    "# Este c√≥digo imprime la cantidad de elementos (hijos) que tiene cada una de las primeras filas de la tabla, permitiendo comparar la estructura de filas no relevantes y relevantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps good rows have a length of 5. Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar las filas que tienen exactamente 5 elementos (columnas)\n",
    "# Esto nos ayuda a identificar las filas que contienen datos v√°lidos de senadores\n",
    "# ya que las filas con informaci√≥n completa deben tener: nombre, foto, comit√©s, distrito y partido\n",
    "good_rows = [row for row in rows if len(row) == 5]\n",
    "\n",
    "# Verificar algunas filas para confirmar que el filtrado funciona correctamente\n",
    "# Imprimir la primera fila v√°lida\n",
    "print(good_rows[0], '\\n')\n",
    "# Imprimir la pen√∫ltima fila v√°lida\n",
    "print(good_rows[-2], '\\n')\n",
    "# Imprimir la √∫ltima fila v√°lida\n",
    "print(good_rows[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found a footer row in our list that we'd like to avoid. Let's try something else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecciona todas las celdas <td> que tienen la clase 'detail' en la tercera fila de la tabla (rows[2])\n",
    "rows[2].select('td.detail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intentamos filtrar las filas relevantes de la tabla usando selectores CSS.\n",
    "\n",
    "# 1. Revisamos una fila \"mala\" (por ejemplo, el pie de p√°gina o filas vac√≠as)\n",
    "print(rows[-1].select('td.detail'), '\\n')  # Normalmente devuelve una lista vac√≠a porque no tiene celdas con la clase 'detail'\n",
    "\n",
    "# 2. Revisamos una fila \"buena\" (una que s√≠ contiene datos de senadores)\n",
    "print(rows[5].select('td.detail'), '\\n')  # Devuelve una lista con celdas que tienen la clase 'detail' (datos √∫tiles)\n",
    "\n",
    "# 3. Filtramos todas las filas que contienen al menos una celda con la clase 'detail'\n",
    "good_rows = [row for row in rows if row.select('td.detail')]\n",
    "\n",
    "print(\"Checking rows...\\n\")\n",
    "print(good_rows[0], '\\n')      # Imprime la primera fila v√°lida (con datos de senador)\n",
    "print(good_rows[-1])           # Imprime la √∫ltima fila v√°lida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we found something that worked!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop it All Together\n",
    "\n",
    "Now that we've seen how to get the data we want from one row, as well as filter out the rows we don't want, let's put it all together into a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definir la lista donde se almacenar√°n los datos de los senadores\n",
    "members = []\n",
    "\n",
    "# Eliminar las filas que no contienen datos relevantes (solo filas con celdas 'td.detail')\n",
    "valid_rows = [row for row in rows if row.select('td.detail')]\n",
    "\n",
    "# Recorrer todas las filas v√°lidas\n",
    "for row in valid_rows:\n",
    "    # Seleccionar solo las celdas 'td' con clase 'detail'\n",
    "    detail_cells = row.select('td.detail')\n",
    "    # Obtener solo el texto de cada celda\n",
    "    row_data = [cell.text for cell in detail_cells]\n",
    "    # Extraer la informaci√≥n relevante: nombre, distrito y partido\n",
    "    name = row_data[0]\n",
    "    district = int(row_data[3])\n",
    "    party = row_data[4]\n",
    "    # Guardar los datos en una tupla\n",
    "    senator = (name, district, party)\n",
    "    # Agregar la tupla a la lista de miembros\n",
    "    members.append(senator)\n",
    " #   *Este bloque de c√≥digo recorre las filas relevantes de la tabla HTML, extrae el nombre, distrito y partido de cada senador, y almacena esa informaci√≥n en una lista de tuplas para su posterior an√°lisis o uso.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debe ser 61 (n√∫mero esperado de miembros del senado)\n",
    "len(members)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what we have in `members`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime los primeros 5 elementos de la lista 'members', que contienen informaci√≥n sobre los senadores\n",
    "print(members[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä  Challenge: Get `href` elements pointing to members' bills \n",
    "\n",
    "The code above retrieves information on:  \n",
    "\n",
    "- the senator's name,\n",
    "- their district number,\n",
    "- and their party.\n",
    "\n",
    "We now want to retrieve the URL for each senator's list of bills. Each URL will follow a specific format. \n",
    "\n",
    "The format for the list of bills for a given senator is:\n",
    "\n",
    "`http://www.ilga.gov/senate/SenatorBills.asp?GA=98&MemberID=[MEMBER_ID]&Primary=True`\n",
    "\n",
    "to get something like:\n",
    "\n",
    "`http://www.ilga.gov/senate/SenatorBills.asp?MemberID=1911&GA=98&Primary=True`\n",
    "\n",
    "in which `MEMBER_ID=1911`. \n",
    "\n",
    "You should be able to see that, unfortunately, `MEMBER_ID` is not currently something pulled out in our scraping code.\n",
    "\n",
    "Your initial task is to modify the code above so that we also **retrieve the full URL which points to the corresponding page of primary-sponsored bills**, for each member, and return it along with their name, district, and party.\n",
    "\n",
    "Tips: \n",
    "\n",
    "* To do this, you will want to get the appropriate anchor element (`<a>`) in each legislator's row of the table. You can again use the `.select()` method on the `row` object in the loop to do this ‚Äî similar to the command that finds all of the `td.detail` cells in the row. Remember that we only want the link to the legislator's bills, not the committees or the legislator's profile page.\n",
    "* The anchor elements' HTML will look like `<a href=\"/senate/Senator.asp/...\">Bills</a>`. The string in the `href` attribute contains the **relative** link we are after. You can access an attribute of a BeatifulSoup `Tag` object the same way you access a Python dictionary: `anchor['attributeName']`. See the <a href=\"http://www.crummy.com/software/BeautifulSoup/bs4/doc/#tag\">documentation</a> for more details.\n",
    "* There are a _lot_ of different ways to use BeautifulSoup to get things done. whatever you need to do to pull the `href` out is fine.\n",
    "\n",
    "The code has been partially filled out for you. Fill it in where it says `#YOUR CODE HERE`. Save the path into an object called `full_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a GET request\n",
    "req = requests.get('http://www.ilga.gov/senate/default.asp?GA=98')\n",
    "# Read the content of the server‚Äôs response\n",
    "src = req.text\n",
    "# Soup it\n",
    "soup = BeautifulSoup(src, \"lxml\")\n",
    "# Create empty list to store our data\n",
    "members = []\n",
    "\n",
    "# Returns every ‚Äòtr tr tr‚Äô css selector in the page\n",
    "rows = soup.select('tr tr tr')\n",
    "# Get rid of junk rows (solo filas con celdas 'td.detail')\n",
    "rows = [row for row in rows if row.select('td.detail')]\n",
    "\n",
    "# Loop through all rows\n",
    "for row in rows:\n",
    "    # Select only those 'td' tags with class 'detail'\n",
    "    detail_cells = row.select('td.detail') \n",
    "    # Keep only the text in each of those cells\n",
    "    row_data = [cell.text for cell in detail_cells]\n",
    "    # Collect information\n",
    "    name = row_data[0]\n",
    "    district = int(row_data[3])\n",
    "    party = row_data[4]\n",
    "\n",
    "    # Buscar el enlace a los proyectos de ley del senador\n",
    "    # El enlace suele estar en la celda con el texto \"Bills\"\n",
    "    bills_link = row.find('a', string='Bills')\n",
    "    if bills_link and bills_link.has_attr('href'):\n",
    "        # Construir la URL completa usando el dominio base\n",
    "        full_path = 'http://www.ilga.gov' + bills_link['href']\n",
    "    else:\n",
    "        full_path = ''\n",
    "\n",
    "    # Store in a tuple\n",
    "    senator = (name, district, party, full_path)\n",
    "    # Append to list\n",
    "    members.append(senator)\n",
    "\n",
    "# Explicaci√≥n:\n",
    "# Este c√≥digo realiza una petici√≥n HTTP para obtener la p√°gina de senadores de Illinois,\n",
    "# la analiza con BeautifulSoup, filtra las filas relevantes de la tabla,\n",
    "# extrae el nombre, distrito y partido de cada senador,\n",
    "# y adem√°s obtiene el enlace directo a los proyectos de ley (\"Bills\") de cada senador.\n",
    "# Finalmente, guarda toda la informaci√≥n en una lista de tuplas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment to test \n",
    "# members[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä  Challenge: Modularize Your Code\n",
    "\n",
    "Turn the code above into a function that accepts a URL, scrapes the URL for its senators, and returns a list of tuples containing information about each senator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TU C√ìDIGO AQU√ç\n",
    "def get_members(url):\n",
    "    # Realiza una solicitud GET a la URL proporcionada y obtiene el contenido HTML\n",
    "    req = requests.get(url)\n",
    "    src = req.text\n",
    "    # Analiza el HTML usando BeautifulSoup con el parser 'lxml'\n",
    "    soup = BeautifulSoup(src, \"lxml\")\n",
    "    # Crea una lista vac√≠a para almacenar los datos de los miembros\n",
    "    members = []\n",
    "    # Selecciona todas las filas relevantes de la tabla usando el selector CSS 'tr tr tr'\n",
    "    rows = soup.select('tr tr tr')\n",
    "    # Filtra solo las filas que contienen al menos una celda <td> con clase 'detail'\n",
    "    rows = [row for row in rows if row.select('td.detail')]\n",
    "    # Itera sobre todas las filas v√°lidas\n",
    "    for row in rows:\n",
    "        # Selecciona √∫nicamente las celdas <td> que tienen la clase 'detail'\n",
    "        detail_cells = row.select('td.detail')\n",
    "        # Extrae solo el texto de cada una de esas celdas\n",
    "        row_data = [cell.text for cell in detail_cells]\n",
    "        # Obtiene el nombre del senador (primer elemento)\n",
    "        name = row_data[0]\n",
    "        # Obtiene el distrito del senador (cuarto elemento) y lo convierte a entero\n",
    "        district = int(row_data[3])\n",
    "        # Obtiene el partido del senador (quinto elemento)\n",
    "        party = row_data[4]\n",
    "        # Busca el enlace a la lista de proyectos de ley del senador\n",
    "        # Busca el enlace cuyo texto sea 'Bills'\n",
    "        bill_link = row.find('a', string='Bills')\n",
    "        # Si se encuentra el enlace, construye la URL completa\n",
    "        if bill_link and bill_link.has_attr('href'):\n",
    "            full_path = 'http://www.ilga.gov' + bill_link['href']\n",
    "        else:\n",
    "            full_path = ''\n",
    "        # Almacena la informaci√≥n en una tupla\n",
    "        senator = (name, district, party, full_path)\n",
    "        # Agrega la tupla a la lista de miembros\n",
    "        members.append(senator)\n",
    "    # Devuelve la lista de miembros\n",
    "    return members\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prueba tu c√≥digo\n",
    "# Define la URL de la p√°gina del Senado de Illinois para la 98¬™ Asamblea General\n",
    "url = 'http://www.ilga.gov/senate/default.asp?GA=98'\n",
    "# Llama a la funci√≥n get_members para obtener la lista de senadores\n",
    "senate_members = get_members(url)\n",
    "# Imprime la cantidad de senadores encontrados\n",
    "len(senate_members)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Take-home Challenge: Writing a Scraper Function\n",
    "\n",
    "We want to scrape the webpages corresponding to bills sponsored by each bills.\n",
    "\n",
    "Write a function called `get_bills(url)` to parse a given bills URL. This will involve:\n",
    "\n",
    "  - requesting the URL using the <a href=\"http://docs.python-requests.org/en/latest/\">`requests`</a> library\n",
    "  - using the features of the `BeautifulSoup` library to find all of the `<td>` elements with the class `billlist`\n",
    "  - return a _list_ of tuples, each with:\n",
    "      - description (2nd column)\n",
    "      - chamber (S or H) (3rd column)\n",
    "      - the last action (4th column)\n",
    "      - the last action date (5th column)\n",
    "      \n",
    "This function has been partially completed. Fill in the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bills(url):\n",
    "    # Realiza una solicitud HTTP GET a la URL proporcionada y obtiene el contenido HTML como texto\n",
    "    src = requests.get(url).text\n",
    "    \n",
    "    # Analiza el contenido HTML usando BeautifulSoup\n",
    "    soup = BeautifulSoup(src)\n",
    "\n",
    "    # Selecciona todos los elementos <tr> del documento (filas de una tabla)\n",
    "    rows = soup.select('tr')\n",
    "\n",
    "    # Inicializa una lista vac√≠a para almacenar las tuplas con la informaci√≥n de cada proyecto de ley\n",
    "    bills = []\n",
    "\n",
    "    # Itera sobre cada fila encontrada\n",
    "    for row in rows:\n",
    "        # TU C√ìDIGO AQU√ç:\n",
    "        # Aqu√≠ deber√≠as extraer las celdas <td> de la fila y luego extraer la informaci√≥n de cada campo.\n",
    "        # Por ejemplo, podr√≠as hacer algo como:\n",
    "        # cols = row.find_all('td')\n",
    "        # if len(cols) >= 5:\n",
    "        #     bill_id = cols[0].text.strip()\n",
    "        #     description = cols[1].text.strip()\n",
    "        #     chamber = cols[2].text.strip()\n",
    "        #     last_action = cols[3].text.strip()\n",
    "        #     last_action_date = cols[4].text.strip()\n",
    "        #     bill = (bill_id, description, chamber, last_action, last_action_date)\n",
    "        #     bills.append(bill)\n",
    "\n",
    "        bill_id =        # Aqu√≠ debes extrae_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment to test your code\n",
    "# test_url = senate_members[0][3]\n",
    "# get_bills(test_url)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape All Bills\n",
    "\n",
    "Finally, create a dictionary `bills_dict` which maps a district number (the key) onto a list of bills (the value) coming from that district. You can do this by looping over all of the senate members in `members_dict` and calling `get_bills()` for each of their associated bill URLs.\n",
    "\n",
    "**NOTE:** please call the function `time.sleep(1)` for each iteration of the loop, so that we don't destroy the state's web site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment to test your code\n",
    "# bills_dict[52]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
